{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10f3dc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import PyPDF2\n",
    "import markdown\n",
    "import html2text\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, Code, Markdown\n",
    "\n",
    "# 导入 python-dotenv 来读取 .env 文件\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载 .env 文件中的环境变量\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02a0fd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境变量加载成功！\n",
      "API密钥前缀: sk-proj-R_...\n",
      "Base URL: https://ai.devtool.tech/proxy/v1\n"
     ]
    }
   ],
   "source": [
    "# 从环境变量获取API密钥\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 检查环境变量是否正确加载\n",
    "if not api_key:\n",
    "    raise ValueError(\"请设置 OPENAI_API_KEY 环境变量\")\n",
    "if not base_url:\n",
    "    raise ValueError(\"请设置 OPENAI_BASE_URL 环境变量\")\n",
    "\n",
    "print(\"环境变量加载成功！\")\n",
    "print(f\"API密钥前缀: {api_key[:10]}...\")\n",
    "print(f\"Base URL: {base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da6b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI 客户端创建成功！\n"
     ]
    }
   ],
   "source": [
    "# 实例化客户端 - 使用环境变量\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "print(\"OpenAI 客户端创建成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "878dd215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境变量通过 .env 文件自动加载\n"
     ]
    }
   ],
   "source": [
    "# 环境变量已通过 .env 文件加载，无需手动设置\n",
    "print(\"环境变量通过 .env 文件自动加载\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb9b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0060808719135820866, 0.01937379501760006, 0.01978626847267151, -0.04902195185422897, 0.005962129216641188, -0.05264672636985779, -0.026648342609405518, -0.012949194759130478, 0.0021545535419136286, -0.02132367342710495]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用 embedding API 获取文本的向量表示\n",
    "response = client.embeddings.create(\n",
    "    input=\"测试文本\",  # 输入文本\n",
    "    model=\"text-embedding-3-small\"  # 选择 Embedding 模型\n",
    ")\n",
    "\n",
    "# 打印返回的embedding向量\n",
    "print(response.data[0].embedding[:10])\n",
    "\n",
    "# 获取embedding长度\n",
    "len(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "651a959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbeddings:\n",
    "    \"\"\"\n",
    "    向量化的基类，用于将文本转换为向量表示。不同的子类可以实现不同的向量获取方法。\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str, is_api: bool) -> None:\n",
    "        \"\"\"\n",
    "        初始化基类。\n",
    "        \n",
    "        参数：\n",
    "        path (str) - 如果是本地模型，path 表示模型路径；如果是API模式，path可以为空\n",
    "        is_api (bool) - 表示是否使用API调用，如果为True表示通过API获取Embedding\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.is_api = is_api\n",
    "    \n",
    "    def get_embedding(self, text: str, model: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        抽象方法，用于获取文本的向量表示，具体实现需要在子类中定义。\n",
    "        \n",
    "        参数：\n",
    "        text (str) - 需要转换为向量的文本\n",
    "        model (str) - 所使用的模型名称\n",
    "        \n",
    "        返回：\n",
    "        list[float] - 文本的向量表示\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_similarity(vector1: List[float], vector2: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        计算两个向量之间的余弦相似度，用于衡量它们的相似程度。\n",
    "        \n",
    "        参数：\n",
    "        vector1 (list[float]) - 第一个向量\n",
    "        vector2 (list[float]) - 第二个向量\n",
    "        \n",
    "        返回：\n",
    "        float - 余弦相似度值，范围从 -1 到 1，越接近 1 表示向量越相似\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(vector1, vector2)  # 向量点积\n",
    "        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)  # 向量的模\n",
    "        if not magnitude:\n",
    "            return 0\n",
    "        return dot_product / magnitude  # 计算余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f08a01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIEmbedding(BaseEmbeddings):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的 Embedding API 来获取文本向量的类，继承自 BaseEmbeddings。\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str = '', is_api: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        初始化类，设置 OpenAI API 客户端，如果使用的是 API 调用。\n",
    "        \n",
    "        参数：\n",
    "        path (str) - 本地模型的路径，使用API时可以为空\n",
    "        is_api (bool) - 是否通过 API 获取 Embedding，默认为 True\n",
    "        \"\"\"\n",
    "        super().__init__(path, is_api)\n",
    "        if self.is_api:\n",
    "            # 初始化 OpenAI API 客户端\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI()\n",
    "            self.client.api_key = os.getenv(\"OPENAI_API_KEY\")  # 从环境变量中获取 API 密钥\n",
    "            self.client.base_url = os.getenv(\"OPENAI_BASE_URL\")  # 从环境变量中获取 API 基础URL\n",
    "    \n",
    "    def get_embedding(self, text: str, model: str = \"text-embedding-3-large\") -> List[float]:\n",
    "        \"\"\"\n",
    "        使用 OpenAI 的 Embedding API 获取文本的向量表示。\n",
    "        \n",
    "        参数：\n",
    "        text (str) - 需要转化为向量的文本\n",
    "        model (str) - 使用的 Embedding 模型名称，默认为 'text-embedding-3-large'\n",
    "        \n",
    "        返回：\n",
    "        list[float] - 文本的向量表示\n",
    "        \"\"\"\n",
    "        if self.is_api:\n",
    "            # 去掉文本中的换行符，保证输入格式规范\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            # 调用 OpenAI API 获取文本的向量表示\n",
    "            return self.client.embeddings.create(\n",
    "                input=[text], \n",
    "                model=model).data[0].embedding\n",
    "        else:\n",
    "            raise NotImplementedError  # 如果不是 API 模式，这里未实现本地模型的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05a970ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本的向量表示为： [0.02648121863603592, 0.013051084242761135, -0.007090849336236715, -0.010436945594847202, 0.01947532780468464, 0.013410528190433979, -0.0019606035202741623, 0.03531700372695923, 0.001083233393728733, -0.026468146592378616] 长度为： 3072\n"
     ]
    }
   ],
   "source": [
    "# 初始化 Embedding 模型\n",
    "embedding_model = OpenAIEmbedding()\n",
    "\n",
    "# 输入需要获取向量表示的文本\n",
    "text = \"这是一个示例文本，用于演示 OpenAI Embedding 的使用。\"\n",
    "\n",
    "# 获取文本的向量表示\n",
    "embedding_vector = embedding_model.get_embedding(text, model=\"text-embedding-3-large\")\n",
    "\n",
    "print(\"文本的向量表示为：\", embedding_vector[:10], \"长度为：\", len(embedding_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "760f353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "两段文本的余弦相似度为: 0.7661800739633337\n"
     ]
    }
   ],
   "source": [
    "text1 = '我喜欢吃苹果'\n",
    "text2 = \"苹果是我最喜欢吃的水果\"\n",
    "text3 = \"我喜欢用苹果手机\"\n",
    "\n",
    "vector1 = embedding_model.get_embedding(text1)\n",
    "vector2 = embedding_model.get_embedding(text2)\n",
    "\n",
    "similarity = OpenAIEmbedding.cosine_similarity(vector1, vector2)\n",
    "print(f\"两段文本的余弦相似度为: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b56d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8533f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadFiles:\n",
    "    \"\"\"\n",
    "    读取文件的类，用于从指定路径读取支持的文件类型（如 .txt、.md、.pdf）并进行内容分割。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        初始化函数，设定要读取的文件路径，并获取该路径下所有符合要求的文件。\n",
    "        :param path: 文件夹路径\n",
    "        \"\"\"\n",
    "        self._path = path\n",
    "        self.file_list = self.get_files()  # 获取文件列表\n",
    "\n",
    "    def get_files(self):\n",
    "        \"\"\"\n",
    "        遍历指定文件夹，获取支持的文件类型列表（txt, md, pdf）。\n",
    "        :return: 文件路径列表\n",
    "        \"\"\"\n",
    "        file_list = []\n",
    "        for filepath, dirnames, filenames in os.walk(self._path):\n",
    "            # os.walk 函数将递归遍历指定文件夹\n",
    "            for filename in filenames:\n",
    "                # 根据文件后缀筛选支持的文件类型\n",
    "                if filename.endswith(\".md\"):\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "                elif filename.endswith(\".txt\"):\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "                elif filename.endswith(\".pdf\"):\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "        return file_list\n",
    "\n",
    "    def get_content(self, max_token_len: int = 600, cover_content: int = 150):\n",
    "        \"\"\"\n",
    "        读取文件内容并进行分割，将长文本切分为多个块。\n",
    "        :param max_token_len: 每个文档片段的最大 Token 长度\n",
    "        :param cover_content: 在每个片段之间重叠的 Token 长度\n",
    "        :return: 切分后的文档片段列表\n",
    "        \"\"\"\n",
    "        docs = []\n",
    "        for file in self.file_list:\n",
    "            content = self.read_file_content(file)  # 读取文件内容\n",
    "            # 分割文档为多个小块\n",
    "            chunk_content = self.get_chunk(content, max_token_len=max_token_len, cover_content=cover_content)\n",
    "            docs.extend(chunk_content)\n",
    "        return docs\n",
    "\n",
    "    @classmethod\n",
    "    def get_chunk(cls, text: str, max_token_len: int = 600, cover_content: int = 150):\n",
    "        \"\"\"\n",
    "        将文档内容按最大 Token 长度进行切分。\n",
    "        :param text: 文档内容\n",
    "        :param max_token_len: 每个片段的最大 Token 长度\n",
    "        :param cover_content: 重叠的内容长度\n",
    "        :return: 切分后的文档片段列表\n",
    "        \"\"\"\n",
    "        chunk_text = []\n",
    "        curr_len = 0\n",
    "        curr_chunk = ''\n",
    "        token_len = max_token_len - cover_content\n",
    "        lines = text.splitlines()  # 以换行符分割文本为行\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.replace(' ', '')  # 去除空格\n",
    "            line_len = len(enc.encode(line))  # 计算当前行的 Token 长度\n",
    "            if line_len > max_token_len:\n",
    "                # 如果单行长度超过限制，将其分割为多个片段\n",
    "                num_chunks = (line_len + token_len - 1) // token_len\n",
    "                for i in range(num_chunks):\n",
    "                    start = i * token_len\n",
    "                    end = start + token_len\n",
    "                    # 防止跨单词分割\n",
    "                    while not line[start:end].rstrip().isspace():\n",
    "                        start += 1\n",
    "                        end += 1\n",
    "                        if start >= line_len:\n",
    "                            break\n",
    "                    curr_chunk = curr_chunk[-cover_content:] + line[start:end]\n",
    "                    chunk_text.append(curr_chunk)\n",
    "                start = (num_chunks - 1) * token_len\n",
    "                curr_chunk = curr_chunk[-cover_content:] + line[start:end]\n",
    "                chunk_text.append(curr_chunk)\n",
    "            elif curr_len + line_len <= token_len:\n",
    "                # 当前片段长度未超过限制时，继续累加\n",
    "                curr_chunk += line + '\\n'\n",
    "                curr_len += line_len + 1\n",
    "            else:\n",
    "                chunk_text.append(curr_chunk)  # 保存当前片段\n",
    "                curr_chunk = curr_chunk[-cover_content:] + line\n",
    "                curr_len = line_len + cover_content\n",
    "\n",
    "        if curr_chunk:\n",
    "            chunk_text.append(curr_chunk)\n",
    "\n",
    "        return chunk_text\n",
    "\n",
    "    @classmethod\n",
    "    def read_file_content(cls, file_path: str):\n",
    "        \"\"\"\n",
    "        读取文件内容，根据文件类型选择不同的读取方式。\n",
    "        :param file_path: 文件路径\n",
    "        :return: 文件内容\n",
    "        \"\"\"\n",
    "        if file_path.endswith('.pdf'):\n",
    "            return cls.read_pdf(file_path)\n",
    "        elif file_path.endswith('.md'):\n",
    "            return cls.read_markdown(file_path)\n",
    "        elif file_path.endswith('.txt'):\n",
    "            return cls.read_text(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    @classmethod\n",
    "    def read_pdf(cls, file_path: str):\n",
    "        \"\"\"\n",
    "        读取 PDF 文件内容。\n",
    "        :param file_path: PDF 文件路径\n",
    "        :return: PDF 文件中的文本内容\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text()\n",
    "            return text\n",
    "\n",
    "    @classmethod\n",
    "    def read_markdown(cls, file_path: str):\n",
    "        \"\"\"\n",
    "        读取 Markdown 文件内容，并将其转换为纯文本。\n",
    "        :param file_path: Markdown 文件路径\n",
    "        :return: 纯文本内容\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            md_text = file.read()\n",
    "            html_text = markdown.markdown(md_text)\n",
    "            # 使用 BeautifulSoup 从 HTML 中提取纯文本\n",
    "            soup = BeautifulSoup(html_text, 'html.parser')\n",
    "            plain_text = soup.get_text()\n",
    "            # 使用正则表达式移除网址链接\n",
    "            text = re.sub(r'http\\S+', '', plain_text) \n",
    "            return text\n",
    "\n",
    "    @classmethod\n",
    "    def read_text(cls, file_path: str):\n",
    "        \"\"\"\n",
    "        读取普通文本文件内容。\n",
    "        :param file_path: 文本文件路径\n",
    "        :return: 文件内容\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "813deea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "支持的文件列表： ['./data/【人工智能】AI构建者手册2025 | ICONIQ发布68页报告| AI原生公司 | AI赋能公司 | 代理工作流 | 基础设施 | 市场定价 | 团队结构 | 成本预算 | 内部效率 - YouTube.txt']\n"
     ]
    }
   ],
   "source": [
    "# 初始化 ReadFiles 类，指定文件目录路径\n",
    "file_reader = ReadFiles(path=\"./data\")\n",
    "\n",
    "# 获取目录下所有支持的文件类型\n",
    "file_list = file_reader.get_files()\n",
    "print(\"支持的文件列表：\", file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b20610c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分块后的文档内容： ['`大家好，这里是最佳拍档，我是大飞。不知道大家有没有发现，最近两年AI领域的讨论已经从“要不要做AI”变成了“怎么把AI做好”。从2023年生成式AI爆发开始，到2025年的今天，越来越多的公司不再纠结于AI的可能性，而是埋头在产品开发、团队搭建、成本控制这些实际的问题上。最近，曾经作为管理硅谷众多科技巨头家办的ICONIQ资本，抓住了这个转折点的时机，发布了一份长达68页的报告，拆解了300家正在开发AI产品的软件公司的实战经验。无论是AI原生公司，还是所谓的AI赋能公司，相信它们的踩坑经验、成功做法，都能带给我们很多的启发。今天大飞就来给大家分享一下。\\n\\n', '问题上。最近，曾经作为管理硅谷众多科技巨头家办的ICONIQ资本，抓住了这个转折点的时机，发布了一份长达68页的报告，拆解了300家正在开发AI产品的软件公司的实战经验。无论是AI原生公司，还是所谓的AI赋能公司，相信它们的踩坑经验、成功做法，都能带给我们很多的启发。今天大飞就来给大家分享一下。\\n\\n要聊AI产品的开发，首先我们得搞清楚现在市面上的AI公司到底有哪些类型。根据报告的调研，主要分为两类，分别是AI原生公司和AI赋能公司。AI原生公司指的是核心产品或商业模式完全由AI驱动的公司，它们的价值几乎都来自于模型训练、推理和持续的学习，这类公司在调研中占了32%，它们的特点是产品的迭代速度非常快。报告里提到，只有1%的AI原生公司还停留在产品发布前的阶段，而AI赋能公司则有11%卡在这个阶段；更关键的是，47%的AI原生产品已经进入了规模化的阶段，也就是说产品已经验证了市场契合度，正在快速扩大用户群和基础设施。这背后可能的原因是，AI原生公司在团队构成、基础设施和融资模式上更有优势，能够更快的跳过试错阶段，而AI赋能公司往往需要把AI“嫁接”到现有的工作流中，难免就会遇到更多的阻碍。', '段；更关键的是，47%的AI原生产品已经进入了规模化的阶段，也就是说产品已经验证了市场契合度，正在快速扩大用户群和基础设施。这背后可能的原因是，AI原生公司在团队构成、基础设施和融资模式上更有优势，能够更快的跳过试错阶段，而AI赋能公司往往需要把AI“嫁接”到现有的工作流中，难免就会遇到更多的阻碍。AI赋能公司则分为两种，一种是在旗舰产品中嵌入AI功能，比如给现有的CRM系统加一个AI推荐模块，这类公司占31%；另一种是开发独立于核心业务的AI产品，比如一个做协作工具的公司，额外再推出一个AI写作助手，这类公司占37%。对于这些公司来说，AI更像是一个提升现有产品价值的工具，而不是全部。比如Salesforce、Atlassian这些传统SaaS巨头，现在都在核心产品里面加入了AI功能，目的是提升自动化、个性化和用户的生产力，但是底层的商业模式和用户体验并没有大的改变。这两种路径的差异，从一开始就决定了它们在产品开发、团队搭建甚至成本结构上的不同。\\n\\n', '有产品价值的工具，而不是全部。比如Salesforce、Atlassian这些传统SaaS巨头，现在都在核心产品里面加入了AI功能，目的是提升自动化、个性化和用户的生产力，但是底层的商业模式和用户体验并没有大的改变。这两种路径的差异，从一开始就决定了它们在产品开发、团队搭建甚至成本结构上的不同。\\n\\n接下来，我们就从产品开发的具体环节来看看这些公司都在做什么，以及它们都遇到了什么问题。不管是AI原生还是AI赋能公司，目前最热门的产品类型基本分为两类：代理工作流和应用层产品。其中，有79%的AI原生公司都在做代理工作流。所谓代理工作流，简单来说就是让AI像“代理Agent”一样去自主完成一系列的任务，比如自动处理客户咨询的全流程，从理解问题到查找资料再到生成回复，甚至能够根据用户的反馈来调整策略。除此之外，垂直领域的AI应用和水平领域的AI应用也很受欢迎，AI原生公司中分别有65%和56%在开发这些应用；而AI赋能公司则占比较低，分别是49%和40%。这也反映了两类公司的不同定位，AI原生公司更专注于通过AI来解决具体的业务流程问题，而AI赋能公司则希望通过AI来增强现有平台的通用性。', '策略。除此之外，垂直领域的AI应用和水平领域的AI应用也很受欢迎，AI原生公司中分别有65%和56%在开发这些应用；而AI赋能公司则占比较低，分别是49%和40%。这也反映了两类公司的不同定位，AI原生公司更专注于通过AI来解决具体的业务流程问题，而AI赋能公司则希望通过AI来增强现有平台的通用性。在模型使用上，有80%的公司会依赖第三方AIAPI，比如GPT和Claude。但是高增长的公司明显更“激进”，77%的高增长公司会在现有的基础模型上微调，54%会从头开发专有模型，而其他公司这两个比例分别是61%和32%。为什么会有这种差异呢？高增长公司通常资源更加充足，而且需要为企业客户提供深度的定制化服务，以及根据客户的数据和需求调整模型，这时候微调或者自研就成了必要选项。而资源有限的公司更倾向于直接用第三方API，这样能最快把产品推向市场，减少前期投入。\\n\\n', '发专有模型，而其他公司这两个比例分别是61%和32%。为什么会有这种差异呢？高增长公司通常资源更加充足，而且需要为企业客户提供深度的定制化服务，以及根据客户的数据和需求调整模型，这时候微调或者自研就成了必要选项。而资源有限的公司更倾向于直接用第三方API，这样能最快把产品推向市场，减少前期投入。\\n\\n从模型的提供商来看，OpenAI的GPT模型依然是绝对主流，有95%的全栈AI公司都在使用，其次是Anthropic的Claude和Google的Gemini。值得注意的是，平均每家公司会用2.8个模型，也就是“多模型的策略”越来越普遍。比如，有的公司在处理简单的文本生成时选择用GPT-4，而处理长文档分析时选择用Claude，处理多模态任务时则使用Gemini，这样既能够优化性能，又能够控制成本，还能避免过度依赖单一的供应商。\\n', 'oogle的Gemini。值得注意的是，平均每家公司会用2.8个模型，也就是“多模型的策略”越来越普遍。比如，有的公司在处理简单的文本生成时选择用GPT-4，而处理长文档分析时选择用Claude，处理多模态任务时则使用Gemini，这样既能够优化性能，又能够控制成本，还能避免过度依赖单一的供应商。\\n在选模型的时候，不同场景的优先级也完全不同。如果是面向客户的产品，准确性是绝对第一位的，74%的公司把它排在第一，而成本排在第二，占比为57%，这和去年的情况很不一样。去年成本在客户产品中几乎是最不重要的，今年排名上升，可能是因为像DeepSeek这样的低成本模型出现，让成本成了更为关键的竞争因素。但如果是内部使用的AI工具，成本就成了头号的考量，占比为74%，其次是准确性，和隐私。毕竟内部工具不直接产生收入，控制成本更重要，而且内部数据往往涉及到机密信息，隐私保护自然也就成了重点。\\n', '乎是最不重要的，今年排名上升，可能是因为像DeepSeek这样的低成本模型出现，让成本成了更为关键的竞争因素。但如果是内部使用的AI工具，成本就成了头号的考量，占比为74%，其次是准确性，和隐私。毕竟内部工具不直接产生收入，控制成本更重要，而且内部数据往往涉及到机密信息，隐私保护自然也就成了重点。\\n在训练和适配技术方面，最常用的是检索增强生成RAG和微调fine-tuning，分别有69%和67%的公司在使用。高增长公司还特别喜欢用提示词技术，比如少样本学习和零样本学习，这可能是因为它们需要快速适配不同客户的需求，而提示词技术能够在不重新训练模型的情况下调整输出。和去年相比，用RAG和微调的公司明显增多了。按理说，基础模型越来越强，微调的需求应该减少，但是实际的情况是，企业客户对定制化的要求更高了，这时候微调依然是必要的。\\n', '用提示词技术，比如少样本学习和零样本学习，这可能是因为它们需要快速适配不同客户的需求，而提示词技术能够在不重新训练模型的情况下调整输出。和去年相比，用RAG和微调的公司明显增多了。按理说，基础模型越来越强，微调的需求应该减少，但是实际的情况是，企业客户对定制化的要求更高了，这时候微调依然是必要的。\\n在部署模型的时候，最大的三个难题分别是幻觉现象、可解释性与信任、以及证明投资回报率ROI，分别占比为39%、38%和34%。另外，计算成本和安全也是比较大的两个问题，比如，一个AI产品上线后，用户量突然增加，API的调用费可能飙升，直接吃掉利润；而如果模型被恶意攻击，生成有害内容，还可能面临法律风险。\\n', '署模型的时候，最大的三个难题分别是幻觉现象、可解释性与信任、以及证明投资回报率ROI，分别占比为39%、38%和34%。另外，计算成本和安全也是比较大的两个问题，比如，一个AI产品上线后，用户量突然增加，API的调用费可能飙升，直接吃掉利润；而如果模型被恶意攻击，生成有害内容，还可能面临法律风险。\\n在基础设施方面，大多数公司都选择了“轻资产”的模式，68%的公司完全使用云服务，64%依赖外部AIAPI提供商。这样做的好处很明显，那就是可以减少前期投入，不用自己维护服务器，还能快速上线产品。但是这也带来了新的问题，供应商的选择、服务等级协议，也就是SLA的谈判，以及按调用次数计费的成本管理，都成了战略级别的事情。如果API提供商突然涨价，或者服务中断，整个产品就会受影响，所以很多公司会和供应商签长期协议，甚至考虑备用方案。只有23%的公司采用了云+本地的混合模式，不到10%完全用本地的基础设施，这些公司往往有着一些特殊的需求，比如金融机构因为合规要求，必须把数据放在自己的服务器里；或者对实时性要求极高，比如自动驾驶的AI模型，需要本地计算来减少延迟。', '产品就会受影响，所以很多公司会和供应商签长期协议，甚至考虑备用方案。只有23%的公司采用了云+本地的混合模式，不到10%完全用本地的基础设施，这些公司往往有着一些特殊的需求，比如金融机构因为合规要求，必须把数据放在自己的服务器里；或者对实时性要求极高，比如自动驾驶的AI模型，需要本地计算来减少延迟。', '产品就会受影响，所以很多公司会和供应商签长期协议，甚至考虑备用方案。只有23%的公司采用了云+本地的混合模式，不到10%完全用本地的基础设施，这些公司往往有着一些特殊的需求，比如金融机构因为合规要求，必须把数据放在自己的服务器里；或者对实时性要求极高，比如自动驾驶的AI模型，需要本地计算来减少延迟。在市场方面，现在最流行的定价模式是混合定价，占比为38%，也就是结合订阅制和按使用量、或者按结果付费。比如，基础的功能收月费，超过一定使用量后额外收费，或者根据AI帮客户节省的成本来抽成。AI赋能型的公司大多会把AI当做一个“增值项”，40%放在高端套餐里，33%免费提供，这其实是把AI作为吸引客户升级或防止客户流失的工具，而不是主要的收入来源。比如很多SaaS工具会说“高级版包含AI分析功能”，以此来推动客户从基础版升级。但是报告指出，这种模式可能不会长久，随着AI的成本越来越高，免费提供会压缩产品的利润空间；而用户使用上的差异很大，重度用户用得多，成本高，轻度用户用得少，收入低，这就会让定价变得很尴尬。所以37%的公司计划在未来12个月调整定价，比如转向更为灵活的按使用量计费，或者根据AI带来的具体价值来定价。', '级。但是报告指出，这种模式可能不会长久，随着AI的成本越来越高，免费提供会压缩产品的利润空间；而用户使用上的差异很大，重度用户用得多，成本高，轻度用户用得少，收入低，这就会让定价变得很尴尬。所以37%的公司计划在未来12个月调整定价，比如转向更为灵活的按使用量计费，或者根据AI带来的具体价值来定价。随着AI产品的规模化，透明度也越来越重要。在产品的规模化阶段，25%的公司会提供详细的模型透明度报告，47%会解释AI如何影响结果，而在pre-launch阶段只有6%会提供详细报告，这其实也很合理，说明产品越成熟，客户越会要求知道AI的工作原理，尤其是对于企业客户来说。\\n\\n', 'I带来的具体价值来定价。随着AI产品的规模化，透明度也越来越重要。在产品的规模化阶段，25%的公司会提供详细的模型透明度报告，47%会解释AI如何影响结果，而在pre-launch阶段只有6%会提供详细报告，这其实也很合理，说明产品越成熟，客户越会要求知道AI的工作原理，尤其是对于企业客户来说。\\n\\n在合规方面，只有13%有专门的AI合规团队，29%的公司有正式的AI伦理和治理政策，47%至少遵守GDPR、CCPA这些数据隐私法，说明很多公司还在“应付”合规，而不是主动去建设合规体系。随着各国的AI监管越来越严，这可能会成为未来的风险点。另外，有66%在用human-in-the-loop的方式，确保AI的公平和安全，简单来说就是在关键决策环节让人类审核，比如AI生成的合同，会让律师再检查一遍。\\n', 'R、CCPA这些数据隐私法，说明很多公司还在“应付”合规，而不是主动去建设合规体系。随着各国的AI监管越来越严，这可能会成为未来的风险点。另外，有66%在用human-in-the-loop的方式，确保AI的公平和安全，简单来说就是在关键决策环节让人类审核，比如AI生成的合同，会让律师再检查一遍。\\n在团队方面，公司规模越大，越可能有专门的AI领导，比如首席AI官、机器学习负责人等等。年收入1亿美元以上的公司中，至少50%都有专门的AI领导，而年收入低于1亿美元的公司只有33%。这是因为规模大了，AI业务更复杂，需要有人去统筹战略、技术和合规，而小公司可能会让CTO或产品负责人直接兼管了。\\n', '。\\n在团队方面，公司规模越大，越可能有专门的AI领导，比如首席AI官、机器学习负责人等等。年收入1亿美元以上的公司中，至少50%都有专门的AI领导，而年收入低于1亿美元的公司只有33%。这是因为规模大了，AI业务更复杂，需要有人去统筹战略、技术和合规，而小公司可能会让CTO或产品负责人直接兼管了。\\n在岗位方面，目前最常见的AI岗位是AI/机器学习工程师，88%的公司都有、有数据科学家的公司占比为72%、有AI产品经理的公司占比67%。但是招人很难，AI/机器学习工程师平均要70天才能招到，数据科学家68天，AI产品经理67天，这比普通工程师难招多了，主要是因为合格的人才少，竞争激烈。还有一些新兴的岗位也在崛起，比如提示工程师和AI设计专家，这些岗位需要懂技术又懂业务，所以现在也成了香饽饽。然而，46%的公司觉得招人不够快，主要原因是“缺乏合格的候选人”，其次是成本高和竞争激烈。有一家公司的技术负责人就说，我们想招有大模型部署经验的工程师，但是市场上这样的人太少了，稍微有点经验的，薪资要求比普通工程师高50%以上，还经常被挖墙脚。因此为了应对，很多公司会自己培养人才，比如让现有工程师参加AI培训，或者和高校合作实习项目。平均来看，公司会计划让20%-30%的工程师专注在AI方面，高增长的公司会更高，甚至能达到37%。这说明AI已经从“边缘项目”变成了“核心业务”，需要足够的工程师去投入。', '通工程师高50%以上，还经常被挖墙脚。因此为了应对，很多公司会自己培养人才，比如让现有工程师参加AI培训，或者和高校合作实习项目。平均来看，公司会计划让20%-30%的工程师专注在AI方面，高增长的公司会更高，甚至能达到37%。这说明AI已经从“边缘项目”变成了“核心业务”，需要足够的工程师去投入。', '通工程师高50%以上，还经常被挖墙脚。因此为了应对，很多公司会自己培养人才，比如让现有工程师参加AI培训，或者和高校合作实习项目。平均来看，公司会计划让20%-30%的工程师专注在AI方面，高增长的公司会更高，甚至能达到37%。这说明AI已经从“边缘项目”变成了“核心业务”，需要足够的工程师去投入。从成本方面来看，AI赋能公司的研发预算中，AI的开发成本大概占10%到20%，年收入越高，比例相对越低，2024年1亿美元以上的公司大约为10%到15%，而低于1亿美元的公司为14%。这可能是因为大公司研发基数大，AI只是其中的一部分，小公司则更聚焦，愿意把更多预算投到AI上。不过，这个比例在2025年有了明显的上涨，普遍增加了5%-10%，说明大家都在加大AI投入。那钱都花到哪里了呢？不同阶段，成本的结构也不一样，在产品发布前，57%的AI预算花在了人才上，因为这时候主要是搭团队、做研发；到了规模化阶段，人才成本占比会降到36%，而基础设施和云成本升到22%，模型推理成本升到13%，这是因为用户多了，服务器、API调用这些“可变的成本”也就上来了。其中，API的使用费是最难控制的，70%的公司把它排在第一，其次是推理成本、模型再训练和更新等等。API的使用费难控制，是因为它和用户的使用量直接挂钩，而用户行为是很难预测的，比如突然有个活动，用户量就会暴涨，API费用可能翻倍。为了省钱，41%的公司在用像Llama3这样的开源模型，37%在尝试优化推理效率，比如压缩模型大小，28%（口误）在用模型蒸馏或者量化技术。举个例子，有的公司把大模型从700亿参数压缩到70亿，推理成本降了70%，性能只降了5%，对很多场景来说完全能接受。', '个活动，用户量就会暴涨，API费用可能翻倍。为了省钱，41%的公司在用像Llama3这样的开源模型，37%在尝试优化推理效率，比如压缩模型大小，28%（口误）在用模型蒸馏或者量化技术。举个例子，有的公司把大模型从700亿参数压缩到70亿，推理成本降了70%，性能只降了5%，对很多场景来说完全能接受。模型训练的月成本也会随着产品的成熟度上升，发布前平均为16.3万美元，规模化阶段为150万美元，推理成本涨得更猛，规模化阶段高增长公司每月要花230万美元，普通公司也要110万美元。数据存储和处理也不便宜，规模化阶段的高增长公司每月要花260万和200万美元，普通公司也要花190万和180万美元。从这些数字我们就能看出，AI产品的“规模化成本”很高，不是光把模型开发出来就行的，还得有足够的资金来支撑后期的运营，这也是很多初创公司需要大笔融资的原因。\\n\\n', '普通公司也要110万美元。数据存储和处理也不便宜，规模化阶段的高增长公司每月要花260万和200万美元，普通公司也要花190万和180万美元。从这些数字我们就能看出，AI产品的“规模化成本”很高，不是光把模型开发出来就行的，还得有足够的资金来支撑后期的运营，这也是很多初创公司需要大笔融资的原因。\\n\\n除了开发对外的AI产品以外，公司也在用AI提升内部的效率，这部分的预算在2025年几乎翻倍。对于年收入10亿美元以上的公司来说，内部AI生产力预算从2024年的平均342万美元涨到2025年的604万美元。在公司内部，有70%左右的员工能够接触到AI工具，但是只有50%会持续使用。大公司则更难推动，年收入10亿美元以上的公司，只有44%的员工持续用AI，而小公司则有57%。这可能是因为大公司流程复杂，员工习惯难改，而且数据安全顾虑更多。纽约人寿的首席数据和分析官唐·武（DonVu）就说，只部署工具肯定不行，尤其是大企业，要让员工真的用起来，得培训、找最积极使用AI的员工来带头，最重要的是高管的持续支持，不然很容易不了了之。', '有44%的员工持续用AI，而小公司则有57%。这可能是因为大公司流程复杂，员工习惯难改，而且数据安全顾虑更多。纽约人寿的首席数据和分析官唐·武（DonVu）就说，只部署工具肯定不行，尤其是大企业，要让员工真的用起来，得培训、找最积极使用AI的员工来带头，最重要的是高管的持续支持，不然很容易不了了之。', '有44%的员工持续用AI，而小公司则有57%。这可能是因为大公司流程复杂，员工习惯难改，而且数据安全顾虑更多。纽约人寿的首席数据和分析官唐·武（DonVu）就说，只部署工具肯定不行，尤其是大企业，要让员工真的用起来，得培训、找最积极使用AI的员工来带头，最重要的是高管的持续支持，不然很容易不了了之。在企业的研发部门内部，最常用的AI场景是编码辅助、内容生成和编写助理，以及文档和知识检索。效果最好的也是编码辅助，65%的公司认为它对生产力的提升最大，高增长公司甚至有33%的代码已经是AI写的，普通公司是27%。但是挑战也是明显的，46%的公司说“找不到合适的使用场景”，42%认为“很难证明ROI”。那么，究竟应该如何衡量内部AI的效果呢？75%的公司会跟踪生产力提升，51%会跟踪成本节省，但只有20%会跟踪收入提升，毕竟内部工具通常都不会直接创造收入。具体方法上，14%只跟踪定量指标，16%只跟踪定性指标，30%既跟踪定量又跟踪定性指标，不过，还有17%的公司完全没开始衡量，这其实是很危险的，因为如果不知道效果，就不知道该优化还是放弃。', '%的公司会跟踪生产力提升，51%会跟踪成本节省，但只有20%会跟踪收入提升，毕竟内部工具通常都不会直接创造收入。具体方法上，14%只跟踪定量指标，16%只跟踪定性指标，30%既跟踪定量又跟踪定性指标，不过，还有17%的公司完全没开始衡量，这其实是很危险的，因为如果不知道效果，就不知道该优化还是放弃。在AI的工具栈方面，PyTorch和TensorFlow这两个深度学习框架最流行，加起来占了一半以上的使用量，但是托管平台也不甘示弱，AWSSageMaker和OpenAI的微调服务用的人也很多，说明团队分成两派，一派喜欢用框架，自己掌控整个过程，一派则喜欢用托管服务，省事。HuggingFace的生态和Databricks的MosaicAITraining也在崛起，它们提供了一些更高层的工具，让训练更简单，比如不用自己写复杂的分布式训练代码，直接调用API就行。\\n\\n', '的微调服务用的人也很多，说明团队分成两派，一派喜欢用框架，自己掌控整个过程，一派则喜欢用托管服务，省事。HuggingFace的生态和Databricks的MosaicAITraining也在崛起，它们提供了一些更高层的工具，让训练更简单，比如不用自己写复杂的分布式训练代码，直接调用API就行。\\n\\n在开发方面，LangChain和HuggingFace的工具集最火，因为它们能简化提示词链、批处理和模型接口这些工作，70%的公司还会用私有或自定义的API，说明很多公司会基于公开模型做二次开发，然后封装成自己的API供内部使用。安全工具也越来越受重视，30%的公司会用Guardrails来做安全检查，防止AI生成有害内容，23%会用Vercel的AISDK进行快速部署，这些工具能让应用更稳定、更合规。\\n', '这些工作，70%的公司还会用私有或自定义的API，说明很多公司会基于公开模型做二次开发，然后封装成自己的API供内部使用。安全工具也越来越受重视，30%的公司会用Guardrails来做安全检查，防止AI生成有害内容，23%会用Vercel的AISDK进行快速部署，这些工具能让应用更稳定、更合规。\\n在监控和观测方面，近一半的公司还在用传统的APM工具，比如Datadog、NewRelic，而不是专门的AI监控工具，这可能是因为这些工具已经融入了现有流程，团队不想再学新的工具。但是专门的AI监控工具也在增长，LangSmith和Weights&Biases各有17%的使用率，它们能够跟踪提示词的效果、检测模型漂移，这些是传统工具做不到的。\\n', 'M工具，比如Datadog、NewRelic，而不是专门的AI监控工具，这可能是因为这些工具已经融入了现有流程，团队不想再学新的工具。但是专门的AI监控工具也在增长，LangSmith和Weights&Biases各有17%的使用率，它们能够跟踪提示词的效果、检测模型漂移，这些是传统工具做不到的。\\n在推理优化方面，英伟达的TensorRT和Triton推理服务器加起来占了60%以上，说明英伟达在推理优化领域几乎垄断，毕竟它们的GPU可以说是行业标杆，软件和硬件也配合得好，能把速度和效率做到极致。在非英伟达的方案里，ONNXRuntime占了18%，它的优势是可以跨硬件，在CPU、GPU和其他加速器上都能跑，适合那些不想被英伟达绑定的公司。\\n在数据处理和特征工程方面，ApacheSpark和Kafka是绝对的主力，分别有44%和42%的公司在用，Spark适合大规模的批处理，Kafka适合实时流处理，这两个几乎是大数据处理的标配。但是小规模的数据处理，还是离不开Pandas，有41%的公司在用，它简单灵活，适合快速分析和原型开发。\\n\\n', '\\n在数据处理和特征工程方面，ApacheSpark和Kafka是绝对的主力，分别有44%和42%的公司在用，Spark适合大规模的批处理，Kafka适合实时流处理，这两个几乎是大数据处理的标配。但是小规模的数据处理，还是离不开Pandas，有41%的公司在用，它简单灵活，适合快速分析和原型开发。\\n\\n在编码辅助方面，GitHubCopilot几乎垄断了市场，75%的开发团队在用，它和VSCode深度集成，支持多种语言，而且背后有GitHub的海量代码训练，效果确实好。Cursor排名第二，有50%的公司在用，它更专注于AI驱动的编辑体验，比如实时重构代码，对很多开发者来说很顺手。其他一些工具，比如Codeium、Sourcegraph虽然也有用户，但是份额远不如前两个，说明编码辅助工具已经形成了“双巨头”的格局。\\n', '且背后有GitHub的海量代码训练，效果确实好。Cursor排名第二，有50%的公司在用，它更专注于AI驱动的编辑体验，比如实时重构代码，对很多开发者来说很顺手。其他一些工具，比如Codeium、Sourcegraph虽然也有用户，但是份额远不如前两个，说明编码辅助工具已经形成了“双巨头”的格局。\\n总结一下，从这份报告来看，2025年的AI开发已经进入了“深水区”，不再是拼谁能先做出一个AI功能，而是拼谁能把AI产品做稳定、做合规、做经济，同时搭建起能持续创新的团队和体系。AI原生公司凭借着天生的优势跑得更快，但AI赋能公司通过在现有产品中嵌入AI，也能找到自己的位置。在模型选择上，多模型策略成为了主流，而成本和定制化成了竞争的关键。定价和合规越来越复杂，需要平衡用户体验、成本和监管要求。人才依然是最大的瓶颈，尤其是既懂技术又懂业务的复合型人才。对想要入局的公司来说，有几个教训值得借鉴：首先要明确AI能解决的具体问题，不要为了AI而AI；其次是控制好API的成本，避免规模上去了利润没了；以及尽早搭建合规体系，别等监管来了再补课；最后就是要重视内部AI工具的落地，提升团队效率往往比对外宣传更重要。如今来看，AI已经不再是未来的趋势，而是现在的日常，怎么把它做好，考验的不只是技术能力，还有战略眼光、组织能力和成本意识。希望今天的内容能给大家一些启发，无论是创业还是职场，都能更好地把握AI带来的机会。感谢大家收看本期视频，我们下期再见。']\n"
     ]
    }
   ],
   "source": [
    "# 将文件内容读取并分块\n",
    "document_chunks = file_reader.get_content(max_token_len=600, cover_content=150)\n",
    "print(\"分块后的文档内容：\", document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "682aeaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "890d1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, document: List[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        初始化向量存储类，存储文档和对应的向量表示。\n",
    "        :param document: 文档列表，默认为空。\n",
    "        \"\"\"\n",
    "        if document is None:\n",
    "            document = []\n",
    "        self.document = document  # 存储文档内容\n",
    "        self.vectors = []  # 存储文档的向量表示\n",
    "    \n",
    "    def get_vector(self, EmbeddingModel: BaseEmbeddings) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        使用传入的 Embedding 模型将文档向量化。\n",
    "        :param EmbeddingModel: 传入的用于生成向量的模型（需继承 BaseEmbeddings 类）。\n",
    "        :return: 返回文档对应的向量列表。\n",
    "        \"\"\"\n",
    "        # 遍历所有文档，获取每个文档的向量表示\n",
    "        self.vectors = [EmbeddingModel.get_embedding(doc) for doc in self.document]\n",
    "        return self.vectors\n",
    "    \n",
    "    def persist(self, path: str = 'storage'):\n",
    "        \"\"\"\n",
    "        将文档和对应的向量表示持久化到本地目录中，以便后续加载使用。\n",
    "        :param path: 存储路径，默认为 'storage'。\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)  # 如果路径不存在，创建路径\n",
    "        # 保存向量为 numpy 文件\n",
    "        np.save(os.path.join(path, 'vectors.npy'), self.vectors)\n",
    "        # 将文档内容存储到文本文件中\n",
    "        with open(os.path.join(path, 'documents.txt'), 'w') as f:\n",
    "            for doc in self.document:\n",
    "                f.write(f\"{doc}\\n\")\n",
    "    \n",
    "    def load_vector(self, path: str = 'storage'):\n",
    "        \"\"\"\n",
    "        从本地加载之前保存的文档和向量数据。\n",
    "        :param path: 存储路径，默认为 'storage'。\n",
    "        \"\"\"\n",
    "        # 加载保存的向量数据\n",
    "        self.vectors = np.load(os.path.join(path, 'vectors.npy')).tolist()\n",
    "        # 加载文档内容\n",
    "        with open(os.path.join(path, 'documents.txt'), 'r') as f:\n",
    "            self.document = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        计算两个向量的余弦相似度。\n",
    "        :param vector1: 第一个向量。\n",
    "        :param vector2: 第二个向量。\n",
    "        :return: 返回两个向量的余弦相似度，范围从 -1 到 1。\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
    "        if not magnitude:\n",
    "            return 0\n",
    "        return dot_product / magnitude\n",
    "\n",
    "    def query(self, query: str, EmbeddingModel: BaseEmbeddings, k: int = 1) -> List[str]:\n",
    "        \"\"\"\n",
    "        根据用户的查询文本，检索最相关的文档片段。\n",
    "        :param query: 用户的查询文本。\n",
    "        :param EmbeddingModel: 用于将查询向量化的嵌入模型。\n",
    "        :param k: 返回最相似的文档数量，默认为 1。\n",
    "        :return: 返回最相似的文档列表。\n",
    "        \"\"\"\n",
    "        # 将查询文本向量化\n",
    "        query_vector = EmbeddingModel.get_embedding(query)\n",
    "        # 计算查询向量与每个文档向量的相似度\n",
    "        similarities = [self.get_similarity(query_vector, vector) for vector in self.vectors]\n",
    "        # 获取相似度最高的 k 个文档索引\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        # 返回对应的文档内容\n",
    "        return [self.document[idx] for idx in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "301746e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建向量数据库\n",
    "vector_store = VectorStore(document=document_chunks)\n",
    "\n",
    "# 使用 OpenAI Embedding 模型对文档进行向量化\n",
    "embedding_model = OpenAIEmbedding()\n",
    "\n",
    "# 获取文档向量并存储\n",
    "vector_store.get_vector(embedding_model)\n",
    "\n",
    "# 持久化存储到本地\n",
    "vector_store.persist('storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "772add51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索结果： ['问题上。最近，曾经作为管理硅谷众多科技巨头家办的ICONIQ资本，抓住了这个转折点的时机，发布了一份长达68页的报告，拆解了300家正在开发AI产品的软件公司的实战经验。无论是AI原生公司，还是所谓的AI赋能公司，相信它们的踩坑经验、成功做法，都能带给我们很多的启发。今天大飞就来给大家分享一下。\\n\\n要聊AI产品的开发，首先我们得搞清楚现在市面上的AI公司到底有哪些类型。根据报告的调研，主要分为两类，分别是AI原生公司和AI赋能公司。AI原生公司指的是核心产品或商业模式完全由AI驱动的公司，它们的价值几乎都来自于模型训练、推理和持续的学习，这类公司在调研中占了32%，它们的特点是产品的迭代速度非常快。报告里提到，只有1%的AI原生公司还停留在产品发布前的阶段，而AI赋能公司则有11%卡在这个阶段；更关键的是，47%的AI原生产品已经进入了规模化的阶段，也就是说产品已经验证了市场契合度，正在快速扩大用户群和基础设施。这背后可能的原因是，AI原生公司在团队构成、基础设施和融资模式上更有优势，能够更快的跳过试错阶段，而AI赋能公司往往需要把AI“嫁接”到现有的工作流中，难免就会遇到更多的阻碍。']\n"
     ]
    }
   ],
   "source": [
    "# 模拟用户查询\n",
    "query = \"AI公司分为哪两种类型？\"\n",
    "result = vector_store.query(query, embedding_model)\n",
    "\n",
    "print(\"检索结果：\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "686cb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    \"\"\"\n",
    "    基础模型类，作为所有模型的基类。\n",
    "    包含一些通用的接口，如加载模型、生成回答等。\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str = '') -> None:\n",
    "        self.path = path  # 用于存储模型文件的路径，默认为空。\n",
    "\n",
    "    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n",
    "        \"\"\"\n",
    "        使用模型生成回答的抽象方法。\n",
    "        :param prompt: 用户的提问内容\n",
    "        :param history: 之前的对话历史（字典列表）\n",
    "        :param content: 提供的上下文内容\n",
    "        :return: 模型生成的答案\n",
    "        \"\"\"\n",
    "        pass  # 具体的实现由子类提供\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        加载模型的方法，通常用于本地模型。\n",
    "        \"\"\"\n",
    "        pass  # 如果是 API 模型，可能不需要实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a7daf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT4oChat(BaseModel):\n",
    "    \"\"\"\n",
    "    基于 GPT-4o 模型的对话类，继承自 BaseModel。\n",
    "    主要用于通过 OpenAI API 来生成对话回答。\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key: str, base_url: str = \"https://ai.devtool.tech/proxy/v1\") -> None:\n",
    "        \"\"\"\n",
    "        初始化 GPT-4o 模型。\n",
    "        :param api_key: OpenAI API 的密钥\n",
    "        :param base_url: 用于访问 OpenAI API 的基础 URL，默认为代理 URL\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)  # 初始化 OpenAI 客户端\n",
    "\n",
    "    def chat(self, prompt: str, history: List = [], content: str = '') -> str:\n",
    "        \"\"\"\n",
    "        使用 GPT-4o 生成回答。\n",
    "        :param prompt: 用户的提问\n",
    "        :param history: 之前的对话历史（可选）\n",
    "        :param content: 可参考的上下文信息（可选）\n",
    "        :return: 生成的回答\n",
    "        \"\"\"\n",
    "        # 构建包含问题和上下文的完整提示\n",
    "        full_prompt = PROMPT_TEMPLATE['GPT4o_PROMPT_TEMPLATE'].format(question=prompt, context=content)\n",
    "\n",
    "        # 调用 GPT-4o 模型进行推理\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # 使用 GPT-4o 小型模型\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 返回模型生成的第一个回答\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e390ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = dict(\n",
    "    GPT4o_PROMPT_TEMPLATE=\"\"\"\n",
    "    下面有一个或许与这个问题相关的参考段落，若你觉得参考段落能和问题相关，则先总结参考段落的内容。\n",
    "    若你觉得参考段落和问题无关，则使用你自己的原始知识来回答用户的问题，并且总是使用中文来进行回答。\n",
    "    问题: {question}\n",
    "    可参考的上下文：\n",
    "    ···\n",
    "    {context}\n",
    "    ···\n",
    "    有用的回答:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ab0c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = GPT4oChat(api_key=api_key)\n",
    "question = 'AI公司分为哪两种类型？'\n",
    "answer = chat.chat(question, [], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c881e2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'参考段落讨论了两种类型的AI公司：AI原生公司和AI赋能公司。AI原生公司是指那些核心产品或商业模式完全由AI驱动的公司，它们的价值主要来自模型训练、推理和持续学习。这类公司在市场上占据了32%，其产品迭代速度快，且多已进入规模化阶段。相对而言，AI赋能公司则需将AI技术整合到现有工作流中，面临更多挑战，其中有11%的公司仍停留在产品发布前阶段。\\n\\n综上所述，AI公司主要可以分为这两种类型：AI原生公司和AI赋能公司。'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58070ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "参考段落讨论了两种类型的AI公司：AI原生公司和AI赋能公司。AI原生公司是指那些核心产品或商业模式完全由AI驱动的公司，它们的价值主要来自模型训练、推理和持续学习。这类公司在市场上占据了32%，其产品迭代速度快，且多已进入规模化阶段。相对而言，AI赋能公司则需将AI技术整合到现有工作流中，面临更多挑战，其中有11%的公司仍停留在产品发布前阶段。\n",
       "\n",
       "综上所述，AI公司主要可以分为这两种类型：AI原生公司和AI赋能公司。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
